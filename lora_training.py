# lora_training.py
# AudioLDM1 + LoRA í•™ìŠµ ì½”ë“œ
# Colabì—ì„œ ì‹¤í–‰ ì „ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
# !pip install diffusers transformers accelerate soundfile librosa torch torchaudio peft datasets

import os
import json
import torch
import torch.nn.functional as F
import soundfile as sf
import librosa
import numpy as np
import argparse
from torch.utils.data import Dataset, DataLoader
from diffusers import AudioLDMPipeline
from peft import LoraConfig, get_peft_model
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

def load_json(path):
    """JSON íŒŒì¼ ë¡œë“œ"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def ensure_dir(d):
    """ë””ë ‰í† ë¦¬ ìƒì„±"""
    os.makedirs(d, exist_ok=True)

class AudioRatingDataset(Dataset):
    """
    rating_data_train.jsonì„ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ (3+1 í‰ê°€ë²•)
    ì „ì²´ ë§Œì¡±ë„, í…ìŠ¤íŠ¸ ì í•©ì„±, ìŒì§ˆ, ê°œì„ ì ì„ í¬í•¨
    """
    def __init__(self, json_path, sr=16000, max_length=96000):  # 6ì´ˆ * 16kHz
        self.data = load_json(json_path)
        self.sr = sr
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # í…ìŠ¤íŠ¸ (ê¸¸ì´ ì œí•œ - 512 í† í° ì œí•œ ëŒ€ì‘)
        text = item.get("text", "").strip()
        if len(text) > 500:  # ì•ˆì „í•œ ê¸¸ì´ë¡œ ì œí•œ
            text = text[:500] + "..."
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio_path = item.get("audio_path", "")
        if os.path.exists(audio_path):
            audio, _ = librosa.load(audio_path, sr=self.sr, duration=6.0)
            # ê¸¸ì´ ë§ì¶”ê¸° (íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°)
            if len(audio) < self.max_length:
                audio = np.pad(audio, (0, self.max_length - len(audio)), 'constant')
            else:
                audio = audio[:self.max_length]
        else:
            # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ì˜¤ë””ì˜¤
            audio = np.zeros(self.max_length)
        
        # 3+1 í‰ê°€ ë°ì´í„°
        overall_score = float(item.get("overall_score", 5.0))      # ì „ì²´ ë§Œì¡±ë„ (1~10)
        text_match = float(item.get("text_match", 5.0))           # í…ìŠ¤íŠ¸ ì í•©ì„± (1~10)
        audio_quality = item.get("audio_quality", "Good")         # ìŒì§ˆ (Good/Bad)
        improvement = item.get("improvement", "")                 # ê°œì„ ì  (ì„ íƒì‚¬í•­)
        
        return {
            "text": text,
            "audio": torch.FloatTensor(audio),
            "overall_score": overall_score,
            "text_match": text_match,
            "audio_quality": audio_quality,
            "improvement": improvement,
            "scene_id": item.get("scene_id", idx)
        }

def collate_fn(batch):
    """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ (3+1 í‰ê°€ë²•)"""
    texts = [item["text"] for item in batch]
    audios = torch.stack([item["audio"] for item in batch])
    overall_scores = torch.FloatTensor([item["overall_score"] for item in batch])
    text_matches = torch.FloatTensor([item["text_match"] for item in batch])
    audio_qualities = [item["audio_quality"] for item in batch]
    improvements = [item["improvement"] for item in batch]
    scene_ids = [item["scene_id"] for item in batch]
    
    return {
        "texts": texts,
        "audios": audios,
        "overall_scores": overall_scores,
        "text_matches": text_matches,
        "audio_qualities": audio_qualities,
        "improvements": improvements,
        "scene_ids": scene_ids
    }

def setup_lora_model(base_model, lora_params):
    """
    AudioLDMì— LoRA ì„¤ì • ì ìš©
    """
    # UNetì˜ attention ë ˆì´ì–´ë“¤ì— LoRA ì ìš©
    target_modules = [
        "to_k", "to_q", "to_v", "to_out.0",  # attention layers
        "ff.net.0.proj", "ff.net.2"  # feedforward layers
    ]
    
    lora_config = LoraConfig(
        r=lora_params["r"],  # LoRA rank
        lora_alpha=lora_params["lora_alpha"],
        target_modules=target_modules,
        lora_dropout=lora_params["lora_dropout"],
        bias="none",
        # TaskType.DIFFUSIONì´ ì—†ìœ¼ë¯€ë¡œ ì œê±°í•˜ê±°ë‚˜ ë‹¤ë¥¸ íƒ€ì… ì‚¬ìš©
    )
    
    # UNetì—ë§Œ LoRA ì ìš© (VAE, Text EncoderëŠ” freeze)
    unet_lora = get_peft_model(base_model.unet, lora_config)
    base_model.unet = unet_lora
    
    return base_model

def compute_spectral_loss(predicted_audio, target_audio):
    """ìŠ¤í™íŠ¸ëŸ¼ ë„ë©”ì¸ì—ì„œì˜ ì†ì‹¤ (í…ìŠ¤íŠ¸ ì í•©ì„±ìš©)"""
    # STFTë¥¼ í†µí•œ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë¹„êµ
    pred_stft = torch.stft(predicted_audio, n_fft=1024, hop_length=256, return_complex=True)
    target_stft = torch.stft(target_audio, n_fft=1024, hop_length=256, return_complex=True)
    
    pred_mag = torch.abs(pred_stft)
    target_mag = torch.abs(target_stft)
    
    return F.mse_loss(pred_mag, target_mag)

def compute_noise_suppression_loss(predicted_audio):
    """ë…¸ì´ì¦ˆ ì–µì œ ì†ì‹¤ (ìŒì§ˆ ê°œì„ ìš©)"""
    # ê³ ì£¼íŒŒ ì„±ë¶„ì— í˜ë„í‹°
    stft = torch.stft(predicted_audio, n_fft=1024, hop_length=256, return_complex=True)
    magnitude = torch.abs(stft)
    
    # ê³ ì£¼íŒŒ ì˜ì—­ (ìƒìœ„ 1/3) ì–µì œ
    high_freq_start = magnitude.size(-2) * 2 // 3
    high_freq_energy = torch.mean(magnitude[..., high_freq_start:, :])
    
    return high_freq_energy

def compute_mood_adjustment_loss(predicted_audio, improvement_text):
    """ê°œì„ ì  í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ìœ„ê¸° ì¡°ì • ì†ì‹¤"""
    stft = torch.stft(predicted_audio, n_fft=1024, hop_length=256, return_complex=True)
    magnitude = torch.abs(stft)
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼íŒŒìˆ˜ ì¡°ì •
    if "ì–´ë‘¡" in improvement_text or "dark" in improvement_text.lower():
        # ì €ì£¼íŒŒ ê°•í™”, ê³ ì£¼íŒŒ ì–µì œ
        low_freq_end = magnitude.size(-2) // 3
        low_freq_penalty = 1.0 / (torch.mean(magnitude[..., :low_freq_end, :]) + 1e-6)
        return low_freq_penalty
    elif "ë°" in improvement_text or "bright" in improvement_text.lower():
        # ê³ ì£¼íŒŒ ê°•í™”
        high_freq_start = magnitude.size(-2) * 2 // 3
        high_freq_boost = torch.mean(magnitude[..., high_freq_start:, :])
        return -high_freq_boost  # ìŒìˆ˜ë¡œ í•˜ì—¬ ê³ ì£¼íŒŒ ê°•í™”
    
    return torch.tensor(0.0, device=predicted_audio.device)

def compute_optimized_loss(predicted_audio, target_audio, rating_data):
    """3+1 í‰ê°€ë²• ê¸°ë°˜ ìµœì í™”ëœ ì†ì‹¤ í•¨ìˆ˜"""
    
    # 1. ê¸°ë³¸ ì¬êµ¬ì„± ì†ì‹¤ (ê°€ì¥ ì¤‘ìš”)
    reconstruction_loss = F.mse_loss(predicted_audio, target_audio)
    
    # 2. ì „ì²´ ë§Œì¡±ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (í•µì‹¬!)
    satisfaction_weight = rating_data['overall_score'] / 10.0
    
    # 3. í…ìŠ¤íŠ¸ ì í•©ì„± ì†ì‹¤ (LoRA ëª©í‘œ)
    text_match_penalty = (10.0 - rating_data['text_match']) / 10.0
    spectral_loss = compute_spectral_loss(predicted_audio, target_audio)
    text_match_loss = text_match_penalty * spectral_loss
    
    # 4. ìŒì§ˆ ì†ì‹¤ (ì´ì§„ ë¶„ë¥˜)
    quality_loss = torch.tensor(0.0, device=predicted_audio.device)
    if rating_data['audio_quality'] == "Bad":
        quality_loss = compute_noise_suppression_loss(predicted_audio)
    
    # 5. ê°œì„ ì  ë°˜ì˜ (ì„ íƒì )
    improvement_loss = torch.tensor(0.0, device=predicted_audio.device)
    if rating_data.get('improvement') and rating_data['improvement'].strip():
        improvement_loss = compute_mood_adjustment_loss(predicted_audio, rating_data['improvement'])
    
    # ì´ ì†ì‹¤ (ë§Œì¡±ë„ ê°€ì¤‘ì¹˜ ì ìš©)
    total_loss = satisfaction_weight * (
        1.0 * reconstruction_loss +      # ê¸°ë³¸ ì¬êµ¬ì„± (50%)
        0.6 * text_match_loss +         # í…ìŠ¤íŠ¸ ì í•©ì„± (30%)
        0.3 * quality_loss +            # ìŒì§ˆ (15%)
        0.1 * torch.abs(improvement_loss)  # ê°œì„ ì  (5%)
    )
    
    return total_loss

def improved_lora_loss(model, batch, device):
    """ê°œì„ ëœ LoRA ì†ì‹¤ - ì‹¤ì œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë²„ì „"""
    
    # LoRA íŒŒë¼ë¯¸í„° í™•ì¸
    lora_params = [p for p in model.unet.parameters() if p.requires_grad]
    
    if not lora_params:
        print("Warning: No trainable LoRA parameters found!")
        return torch.tensor(1.0, requires_grad=True, device=device), {}
    
    # ë°°ì¹˜ ë°ì´í„°
    texts = batch["texts"]
    target_audios = batch["audios"].to(device)
    overall_scores = batch["overall_scores"].to(device)
    text_matches = batch["text_matches"].to(device)
    audio_qualities = batch["audio_qualities"]
    improvements = batch["improvements"]
    
    try:
        # 1. í…ìŠ¤íŠ¸ ì¸ì½”ë”© (AudioLDM text encoder ì‚¬ìš©)
        with torch.no_grad():
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            truncated_texts = [text[:200] for text in texts]  # ë” ì§§ê²Œ ì œí•œ
            
        # 2. íƒ€ê²Ÿ ì˜¤ë””ì˜¤ë¥¼ mel-spectrogramìœ¼ë¡œ ë³€í™˜
        mel_specs = []
        for audio in target_audios:
            # librosaë¡œ mel-spectrogram ìƒì„±
            audio_np = audio.detach().cpu().numpy()
            mel_spec = librosa.feature.melspectrogram(
                y=audio_np, 
                sr=16000, 
                n_mels=64,
                hop_length=256,
                n_fft=1024
            )
            mel_spec = torch.from_numpy(mel_spec).to(device)
            mel_specs.append(mel_spec)
        
        mel_specs = torch.stack(mel_specs).unsqueeze(1)  # [batch, 1, mel_bins, time]
        
        # 3. VAEë¥¼ í†µí•œ latent space ë³€í™˜
        with torch.no_grad():
            # mel-spectrogramì„ ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •
            target_size = (64, 312)  # AudioLDM VAE ì…ë ¥ í¬ê¸°
            resized_mels = F.interpolate(mel_specs, size=target_size, mode='bilinear')
            
            # VAE encoderë¡œ latent ë³€í™˜
            target_latents = model.vae.encode(resized_mels).latent_dist.sample()
            target_latents = target_latents * model.vae.config.scaling_factor
        
        # 4. ë…¸ì´ì¦ˆ ì¶”ê°€ (Diffusion forward process)
        batch_size = target_latents.shape[0]
        noise = torch.randn_like(target_latents)
        
        # ë‹¤ì–‘í•œ timestepì—ì„œ í•™ìŠµ
        timesteps = torch.randint(0, model.scheduler.config.num_train_timesteps, (batch_size,), device=device)
        noisy_latents = model.scheduler.add_noise(target_latents, noise, timesteps)
        
        # 5. í…ìŠ¤íŠ¸ ì¡°ê±´í™”ë¥¼ ìœ„í•œ ì„ë² ë”© ìƒì„±
        # AudioLDMì˜ text encoder ì‚¬ìš©
        text_inputs = model.tokenizer(
            truncated_texts,
            padding=True,
            truncation=True,
            max_length=77,
            return_tensors="pt"
        ).to(device)
        
        text_embeddings = model.text_encoder(**text_inputs)[0]
        
        # 6. UNetìœ¼ë¡œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ (LoRA ì ìš©ëœ ìƒíƒœ)
        model_pred = model.unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=text_embeddings
        ).sample
        
        # 7. ê¸°ë³¸ diffusion ì†ì‹¤
        diffusion_loss = F.mse_loss(model_pred, noise)
        
        # 8. í‰ê°€ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
        # ë‚®ì€ ì ìˆ˜ì¼ìˆ˜ë¡ ë” ë§ì´ í•™ìŠµ
        satisfaction_penalty = (10.0 - overall_scores) / 10.0
        text_match_penalty = (10.0 - text_matches) / 10.0
        
        # ìŒì§ˆì´ ë‚˜ìœ ê²½ìš° ì¶”ê°€ í˜ë„í‹°
        quality_penalty = torch.tensor([
            2.0 if quality == "Bad" else 1.0 
            for quality in audio_qualities
        ], device=device)
        
        # ê°œì„ ì‚¬í•­ì´ ìˆëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ì¦ê°€
        improvement_weight = torch.tensor([
            1.5 if improvement.strip() else 1.0 
            for improvement in improvements
        ], device=device)
        
        # 9. ì¢…í•© ê°€ì¤‘ì¹˜ ê³„ì‚°
        total_weight = (
            satisfaction_penalty * 0.4 +      # ì „ì²´ ë§Œì¡±ë„ 40%
            text_match_penalty * 0.3 +        # í…ìŠ¤íŠ¸ ë§¤ì¹­ 30%
            quality_penalty * 0.2 +           # ìŒì§ˆ 20%
            improvement_weight * 0.1          # ê°œì„ ì‚¬í•­ 10%
        )
        
        # ë°°ì¹˜ë³„ë¡œ ê°€ì¤‘ì¹˜ ì ìš©
        weighted_losses = []
        for i in range(batch_size):
            weighted_loss = total_weight[i] * F.mse_loss(model_pred[i:i+1], noise[i:i+1])
            weighted_losses.append(weighted_loss)
        
        total_loss = torch.stack(weighted_losses).mean()
        
        # 10. ìƒì„¸ ì†ì‹¤ ì •ë³´
        detailed_losses = {
            'total': total_loss,
            'reconstruction': diffusion_loss,
            'satisfaction': overall_scores.mean() / 10.0,
            'text_match': text_matches.mean() / 10.0,
            'avg_weight': total_weight.mean()
        }
        
        return total_loss, detailed_losses
        
    except Exception as e:
        print(f"Improved loss computation error: {e}")
        # ì—ëŸ¬ ì‹œ ê¸°ë³¸ LoRA ì†ì‹¤ë¡œ í´ë°±
        param_changes = sum(p.pow(2).sum() for p in lora_params) / len(lora_params)
        satisfaction_penalty = (10.0 - overall_scores.mean()) / 10.0
        
        fallback_loss = satisfaction_penalty * param_changes
        
        detailed_losses = {
            'total': fallback_loss,
            'reconstruction': param_changes,
            'satisfaction': overall_scores.mean() / 10.0,
            'text_match': text_matches.mean() / 10.0,
            'avg_weight': satisfaction_penalty
        }
        
        return fallback_loss, detailed_losses

def compute_optimized_batch_loss(model, batch, device):
    """
    3+1 í‰ê°€ë²• ê¸°ë°˜ ë°°ì¹˜ ì†ì‹¤ í•¨ìˆ˜ - ê°œì„ ëœ ë²„ì „
    """
    return improved_lora_loss(model, batch, device)

def train_lora(args):
    """
    LoRA í•™ìŠµ ë©”ì¸ í•¨ìˆ˜
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    print("Loading dataset...")
    dataset = AudioRatingDataset(args.train_json, sr=args.sr)
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0  # Colabì—ì„œëŠ” 0ìœ¼ë¡œ ì„¤ì •
    )
    print(f"Dataset loaded: {len(dataset)} samples")
    
    # 2. ëª¨ë¸ ë¡œë“œ
    print("Loading AudioLDM model...")
    model = AudioLDMPipeline.from_pretrained(
        "cvssp/audioldm",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    # 3. LoRA ì„¤ì • ì ìš©
    print("Setting up LoRA...")
    lora_config = {
        "r": args.lora_r,
        "lora_alpha": args.lora_alpha, 
        "lora_dropout": args.lora_dropout
    }
    model = setup_lora_model(model, lora_config)
    
    # 4. ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    trainable_params = [p for p in model.unet.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(trainable_params, lr=args.learning_rate)
    
    # 5. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    total_steps = len(dataloader) * args.num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=total_steps // 10,
        num_training_steps=total_steps
    )
    
    # 6. í•™ìŠµ ë£¨í”„
    print("Starting LoRA training...")
    # AudioLDM íŒŒì´í”„ë¼ì¸ì€ train() ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ UNetë§Œ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    model.unet.train()
    
    for epoch in range(args.num_epochs):
        epoch_loss = 0.0
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.num_epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            try:
                # Forward pass - 3+1 í‰ê°€ë²• ê¸°ë°˜ ì†ì‹¤ ê³„ì‚°
                total_loss, detailed_losses = compute_optimized_batch_loss(model, batch, device)
                
                # Backward pass
                optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
                optimizer.step()
                scheduler.step()
                
                epoch_loss += total_loss.item()
                
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸ (3+1 í‰ê°€ ê¸°ë°˜)
                progress_bar.set_postfix({
                    'total': f"{total_loss.item():.4f}",
                    'recon': f"{detailed_losses['reconstruction'].item():.4f}",
                    'satis': f"{detailed_losses['satisfaction'].item():.3f}",
                    'match': f"{detailed_losses['text_match'].item():.3f}",
                    'avg': f"{epoch_loss/(batch_idx+1):.4f}",
                    'lr': f"{scheduler.get_last_lr()[0]:.2e}"
                })
                
                # ì¤‘ê°„ ì €ì¥ (ë§¤ 100ìŠ¤í…ë§ˆë‹¤)
                if (batch_idx + 1) % 100 == 0:
                    checkpoint_path = os.path.join(args.output_dir, f"checkpoint_epoch{epoch+1}_step{batch_idx+1}")
                    model.unet.save_pretrained(checkpoint_path)
                    print(f"\nCheckpoint saved: {checkpoint_path}")
                
            except Exception as e:
                print(f"Error in batch {batch_idx}: {e}")
                continue
        
        print(f"Epoch {epoch+1} completed. Average loss: {epoch_loss/len(dataloader):.4f}")
    
    # 7. ìµœì¢… ëª¨ë¸ ì €ì¥
    model.unet.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
    ensure_dir(args.output_dir)
    final_model_path = os.path.join(args.output_dir, "lora_weights")
    model.unet.save_pretrained(final_model_path)
    print(f"LoRA training completed! Model saved to: {final_model_path}")
    
    return final_model_path

def main():
    parser = argparse.ArgumentParser(description="AudioLDM LoRA Training")
    
    # ë°ì´í„° ê´€ë ¨
    parser.add_argument("--train_json", default="rating_data_train.json", help="Training data JSON file")
    parser.add_argument("--sr", type=int, default=16000, help="Sample rate")
    
    # LoRA ì„¤ì •
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1, help="LoRA dropout")
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--batch_size", type=int, default=2, help="Batch size")
    parser.add_argument("--num_epochs", type=int, default=10, help="Number of epochs")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="Learning rate")
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument("--output_dir", default="weight", help="Output directory for LoRA weights")
    
    args = parser.parse_args()
    
    # í•™ìŠµ ì‹œì‘
    model_path = train_lora(args)
    print(f"\nâœ… LoRA training completed successfully!")
    print(f"ğŸ“ Model saved at: {model_path}")
    print(f"ğŸš€ You can now use this with lora_generate.py --lora_weights {model_path}")

if __name__ == "__main__":
    main()